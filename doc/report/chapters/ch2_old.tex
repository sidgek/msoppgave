\chapter{State of the Art} \label{chap:related-work}

The terminology surrounding reproducibility and the practice of reproducibility in computer science is reviewed in this chapter. Research documenting empirical evidence of reproducibility in machine learning and computer science is reviewed, with some notable projects from other fields. Additionally, guidelines meant to facilitate computational reproducibility is presented. The review is necessary for the survey to be in line with related work on research reproducibility.

\section{Reproducibility} \label{sec:reproducibility}
Claerbout began experimenting with electronic documents as a means to transparently publish reproducible research in 1990. The effort included publishing a CD-ROM along papers containing software, data and documentation to reproduce the entire process from raw data to figures provided in his papers \citep{Claerbout1992}. Inspired by Claerbout, \cite{Buckheit1995} released WaveLab\footnote{\url{https://statweb.stanford.edu/~wavelab/}} in the early days of the internet. WaveLab was a package containing code and data for their papers, allowing anyone to reproduce their research and inspect, modify and reuse their code and data. They also condense Claerbout's ideas in the slogan \emph{"An article about computational science in a scientific publication is \textbf{not} the scholarship itself, it is merely \textbf{advertising} of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures"} \citep{Buckheit1995}. Making it clear that the process leading up to the presentation is of more importance than the paper, and that reproducible research requires code, data, and documentation of this process to be made available with the paper.

\cite{stodden2011trust} distinguishes between replication and reproduction, stating that "replication using author-provided code and data, and independent reproduction work hand-in-hand". Replication is seen as rerunning the experiment with code and data provided by the author, while reproduction is a broader term \emph{"implying both replication and the regeneration of findings with at least some independence from the [original] code and/or data"}. Replication is a means to resolve differences in reproductions. She argues that transparency in methodology arises from the notion of reproducibility. The openness of data or code is not the goal, but a requirement to achieve verifiable research through reproducibility. \cite{drummond2009replicability} presents a dissenting opinion on the need for reproducibility, stating that replication, as the weakest form of reproducibility, can only achieve checks for fraud. Additionally, he raises the arguments that the majority of published code would not be used based on many papers having none, or few citations, and that the increased reviewer workload is not feasible. Drummond believes increased scepticism to results of experiments would reduce the impact of misconduct. However, there is no contradiction in his belief that replication is a weak form of reproduction and Stodden's view. For Stodden, replication is there to be able to analyse why differences have occurred in later reproductions. It is not there to corroborate the idea or experimental results, which both believe require independent reproductions in different environments. In the event that the original does not differ from reproductions, publishing of the original material to allow replication can still be of great value to facilitate further research through data and code reuse \citep{brown12blog, stodden2014best-practices}. Gent introduces \emph{recomputation} as the ability to replicate a computational experiment, and encourages computational sciences to provide experiments in copies of virtual machines to make them easily recomputable for all time \citep{gent2013recomputation}.

To differentiate between the sources and possible solutions to reproducibility problems, \cite{stodden2013resolving} introduces \emph{empirical} and \emph{computational} reproducibility, adding \emph{statistical} reproducibility in a later publication \citep{stodden2014statistical}. For computational reproducibility traditional scientific publications do not include enough information for computational methods to be reproduced, requiring supplementary material in the form of data, code, and implementation details. Lacking empirical and statistical reproducibility relates to the study power and bias \citep{Ioannidis_2005}, publication bias and ineffective peer review \citep{Francis_2012}, and misapplied methodology \citep{simmons2011false-positive}. These require different remedies than computational reproducibility, where \emph{"issues arise from an exogenous shift in the scientific research process itself - the broad use of computation"} \citep{stodden2014statistical}.

Due to the inconsistencies in the use of the terms replicability and reproducibility, \cite{Goodman341ps12} proposes an extension of the most widely used term, reproduciblility, to be more precise. The proposed terms are \emph{methods} reproducibility, \emph{results} reproducibility and \emph{inferential} reproducibility. They are defined as follows; methods reproducibility - \emph{"the ability to implement, as exactly as possible, the experimental and computational procedures, with the same data and tools, to obtain the same results"}, results reproducibility \emph{"the production of corroborating results in a new study, having used the same experimental methods"}, and lastly inferential reproducibility - \emph{"the drawing of qualitatively similar conclusions from either an independent replication of a study or a reanalysis of the original study"} \citep{Goodman341ps12}. The approach is similar to \cite{stodden2013resolving}, and both results and methods reproducibility are essential for computational reproducibility. Additionally, inferential reproducibility can be facilitated by computational reproducibility - especially in the case of a reanalysis. Goodman highlights that \emph{"reporting of all relevant aspects of scientific design, conduct, measurements, data and analysis"} is necessary for all three types of reproducibility, in line with Stodden's view that availability of the computational environment is necessary for computational reproducibility. The goal of such transparency is, for Goodman, a way to ease evaluation of the weight of evidence from studies to facilitate future studies on actual knowledge gaps and cumulative knowledge, and reduce time spent exploring blind alleys from poorly reported research.

\section{Reproducibility in Practice} \label{sec:practice}
\cite{stodden2010machinelearning} presents a survey on what affects the decision to reveal code, data and ideas among registrants at the NIPS\footnote{Neural Information Processing Systems: \url{https://nips.cc}} conference up to and including 2008. Respondents tend to be steered by communitarian norms when sharing their work, such as one respondent stating that \emph{"Much of my research would have not been possible if other people had not released their data or code. This is one of the main reasons for which I also want to contribute by releasing my own data and code"} \citep{stodden2010machinelearning}. When not sharing material, however, private incentives dominate the decision. Examples of some prominent private incentives are the time necessary to clean up and document before a release and that the data or code may be used without citation. Stodden recorded positive improvements in the data and code sharing policies adopted by journals in \cite{stodden2013journals}.

While attempting to reproduce two NLP technologies, measuring WordNet similarity and Named Entity Recognition (NER), \cite{fokkens2013offspring} identified five main categories that influence reproduction. They note that omitting how \emph{preprocessing} of data is done can break an experiment. The \emph{experimental setup}, which steps to perform and how they are performed, had an impact on the NER experiment where the split of data for cross-validation lead to a major difference in results. Attention to \emph{versioning} is important, as both experiments used datasets with different versions available and regular updates. This was also highlighted in \cite{meng2015invariant}. \emph{System output}, even output from intermediate steps, was found to be critical in the WordNet replication to identify why differences in output occurred. Finally, \emph{system variations} may be inherent in the techniques used, such as how an algorithm reacts to a tie. Their reproductions provide new insights and better understanding of the techniques studied. They acknowledge that sharing of data and software was important for them to reproduce the work. Additionally, they observed that aspects with a huge impact on the results and conclusions of an experiment are often only mentioned in passing, or not at all if it is not the core of the research described \citep{fokkens2013offspring}.

\cite{Collberg2016} studied the willingness of researchers to share data and code at eight ACM\footnote{Association for Computing Machinery: \url{http://www.acm.org/}} conferences and five journals from 2011-13. Out of 402 experimental papers they were able to reproduce 32.3\% without communicating with the author, rising to 48.3\% with communication. Papers by authors with industry affiliation showed a lower rate of reproducibility. Some notable build errors for papers they received distributions for but failed to build were: the distributed code is missing files, a third-party package could not be found, a pre-requisite tool could not be built, or a particular version of software, compiler, etc. was not available. Some of the reasons given for not sharing code for the papers they could not get code for (44\%), were: could not find the final version corresponding to the paper, the code is being worked on for future release (too bad to release), the code was not intended to be released, the programmer was no longer available (student no longer part of program) or the code has restrictive licenses. An argument for why code should be published regardless is presented in \cite{leveque2013top}, liking publishing of source code to the policy of publishing proof along with a mathematical theorem. As a low effort incentive to increase open source and open data, \cite{Collberg2016} propose adding a mandatory sharing specification to the header of papers stating if code and data is accessible or not. The goal is to give the reader or reviewer an idea of how reproducible the research is up front, and create a contract the author has to fulfil. "The contract commits the author to making available certain resources ... and committing the reader/reviewer to take these resources into account when evaluating the contributions made by the paper" \citep{Collberg2016}.

For preclinical medical research, \cite{prinz2011believe} analysed data from 67 target validation projects in the pharmaceutical industry and found inconsistencies in published data and in-house data in almost two-thirds of the projects. The inconsistencies resulted in prolonged validation processes or in most cases termination of the projects as the evidence was insufficient to justify further investments. Further, they found no correlation between reproducibility and the impact factor of the journal the published data originated from. Among the observed reasons for a lack of reproducibility they mention incorrect or inappropriate statistical analysis, insufficient sample sizes, a bias towards publishing positive results and the control or reporting of experimental conditions, such as the description of materials and methods. There are signs that the competitive culture in science, where the number of high-impact-factor publications and grants matter more than the joy of discovery and collaborative contributions, encourages poor scientific practices \citep{Casadevall_2011}. \cite{Begley_2013} provides six questions to recognize whether the data from a preclinical paper is likely to stand up, focusing on the methodology presented. A later review of reproducibility problems in basic and preclinical biomedical research by \cite{begley2015reproducibility} highlights potential solutions to improve research reproducibility. Their recommendations are aimed at funding agencies, institutions, journals and investigators, and require each one to take part. For institutions they mention rewarding robust, rather than flashy studies and to reward researchers who comply with peer-generated guidelines and funding-agency requirements. These go hand in hand with a wish for journals and funding agencies to demand and reward good scientific methods. Generally, their recommendations aim at reducing the perceived need and incentives to publish early by rewarding more robust publications and to label exploratory investigations as exploratory, highlighting that they need further validation.

\section{Facilitating Reproducibility} \label{sec:improvements}

\cite{gent2013recomputation} introduces a recomputation manifesto, using virtual machines to package an experiment with its environment. Additionally, Gent presents a site\footnote{\url{http://recomputation.org}} with a free repository to store virtual machines aimed at recomputing experiments. It has been available since 2013. For the latest conference listed among its library, CP 2015\footnote{\url{http://booleconferences.ucc.ie/cp2015}}, two out of more than 50 accepted papers published their experiments in virtual machines at the site. This initiative is not unique. ReproZip\footnote{\url{https://vida-nyu.github.io/reprozip/}} packs an experiment with its necessary configuration to be easily reproducible on other machines, regardless of dependencies or operating system \citep{chirigati2013reprozip}. General packaging tools like Docker \citep{boettiger2015introduction, nagler2015sustainability} for software distribution or more environment specific tools such as iPython notebooks \citep{PER-GRA:2007} (recently as a kernel for the more general Jupyter notebooks \citep{Kluyver2016}), provide several means to publish reproducible experiments. \cite{yilmaz2012Reproducibility} introduce an e-Portfolio to publish code, data and scientific workflow in an ensemble of active documents. Though the means are available, \cite{gent2014recomputation} observed that the main issue is their ease of use. The tools either have to be used from the very beginning of an experiment, or has a steep learning curve to retrofit the experiment. An attempt at making experiments using large-scale computing clusters easier to reproduce was presented at IEEE BigData 2016 \citep{Monajemi}. Similarly, \cite{leitner2016acrv} proposes the ACR Picking Benchmark\footnote{\url{http://juxi.net/dataset/acrv-picking-benchmark/}}, a robotics benchmark where all items necessary are easily available and common to allow wide spread use and easier reproduction of the benchmark experiment.

\cite{hunold2013state} encourages the addition of a description of how to reproduce the findings in a publication. Highlighting that \emph{"the description of how to reproduce findings should play an important part in every serious, experiment-based parallel computing research article"} \citep{hunold2013state}. By describing how to reproduce findings in a publication, the reader is better equipped to verify the soundness of the experiment. \cite{dolfi2014model} provides a sample paper as a baseline model for other reproducible papers. This includes a guide for reproducing the experiment as an appendix, in line with the wishes of \cite{hunold2013state}. Additionally, the reader is given an idea of evaluation parameters to tweak to evaluate the robustness of the authors' evaluation.

A set of best practice principles for disseminating reproducible computational research is presented in \cite{stodden2014best-practices}. They are based around the idea that any publication of scientific work should include the material necessary to support its major claims, and enable other researchers to verify or reproduce the work. There are six principles, and they all relate to publication of materials supporting the dissemination. The first four consider code, methods, and data used during the experiment. They recommend open licensing of both data and code, and to make it easily accessible (for instance with a digital object identifier), tracking of workflow during the research process and that input values and randomization seeds that generated the results are included with the code. The fifth principle urges researchers to cite any and all 3rd party data and software used. Such citations might provide further incentive to publish material freely, seeing recognition for previous open sourced work. The last principle recommends following conditions associated with funded research, but to make the source of the requirements aware of improvements that could be made to be more in line with the previous principles \citep{stodden2014best-practices}. Ten simple rules proposed by \cite{Sandve_2013} mirror the principles presented by Stodden, with examples of how to implement the rules.
