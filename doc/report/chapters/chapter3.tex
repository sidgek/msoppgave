%===================================== CHAP 3 =================================

\chapter{Research Method}
\label{chap:survey-design}
A literature survey was designed to investigate reproducible research, and its design is outlined in this chapter. Following the design is documentation of the evaluation procedure for each paper, and the chapter ends with a discussion of some known limitations of the survey.

\section{Literature Survey Design}
To investigate the reproducibility of published research, this survey employs manual evaluation of investigated papers, in contrast to previous related research attempting to run executables provided by authors. This decision is based on the ideal that any researcher should be able to attempt a reproduction, or evaluate the design of the research published in a paper. It does not mean that reproductions need to be successful, only that the information necessary to attempt one is made available for peers to critique. The decision to not communicate with authors is in part to save time the time necessary to gather supplementary materials, but also with the notion that the authors might not be available in the future. This notion is to some extent supported by \cite{Collberg2016}, where they found it quite common for the lead developer to be unavailable, due to students graduating, email addresses not working, or authors not having time to help.

A survey strictly based on published documentation has the benefit of allowing any reader to verify and critique evaluations made and the results presented in the future. Using material received from authors might differ based on when it is done and on the availability of the authors. Additionally, attempts to reproduce an experiment will depend on the investigators knowledge of the subject area, the time spent per experiment, and the computational resources available. A literature survey lowers the cost, and allows a larger sample population to analyse.

Disadvantages to a survey, however, includes a shift in focus to what can be counted and measured, as evidenced by the variables in section~\ref{subsec:data-req}. Nuances and aspects not thought of when designing the survey may be overlooked, and the depth of investigation into the research topic is limited.

The survey is an adaptation of the survey presented in \cite{Gundersen2015}, which evaluated 58 papers from the agent track at IJCAI 2013 as well as benchmarks from SHREC 2015.

\subsection{Data requirements}
\label{subsec:data-req}
The survey focuses on reproducibility in line with the terms methods reproducibility and results reproducibility defined by \cite{Goodman341ps12}. Methods reproducibility requires enough information for another researcher to be able to, in theory, exactly perform the same procedures with the same data. As such, the experiment, methods, and data need to be made available. As for results reproducibility, another independent researcher should be able to corroborate the results following the same experimental procedures. What constitutes corroborating results is not well defined, and depends on the experiment. It is assumed that the data is not necessary, as the corroborated results should be in line with the results or analysis presented in the paper.

\subsubsection{Directly topic related}
With methods and results reproducibility in mind, the variables to record for each paper directly related to the topic cover the following categories: experiment documentation, methods documentation, and data documentation. The survey focuses on information available without contact with the authors, meaning resources need to be freely accessible and openly published.

\begin{description}
\item[Experiment documentation]: How well documented is the experiment and the environment it was performed in.
    \begin{description}
    \item[Evaluation criteria] Are the criteria used to evaluate the method described?
    \item[Experiment set-up] Is the set-up for the experiment described? Are hyper-parameters used during the experiment specified?
    \item[Hardware specification] Is the hardware used during the experiment specified?
    \item[Open experiment code] Is the code to run the experiment made available?
    \item[Software dependencies] Are software dependencies listed?
    \end{description}
\item[Method documentation]: Documentation and availability of the method presented.
    \begin{description}
    \item[Pseudo-code] A textual description of the computational methods.
    \item[Open source code] The method source code is accessible.
    \end{description}
\item[Open data]: Documentation and availability of the data used and generated during the experiment.
    \begin{description}
    \item[Open training data]: Training data is available directly or through explicit mention of data split.
    \item[Open validation data]: Validation data is available directly or through explicit mention of data split. Note that simply saying cross-validation was used is not enough, without specifying a type of cross-validation.
    \item[Open test data]: Test data is available directly or through explicit mention of data split.
    \item[Open results data]: Results data is published openly.
    \end{description}
\end{description}

\subsubsection{Indirectly topic related}
To identify a paper and allow different analysis patterns, some indirectly topic related variables are recorded as well. These are sectioned into miscellaneous data, identifying data, and research transparency. Research transparency investigates explicit documentation of a natural-science based research method, to see if research methods in AI overlap with the traditional scientific method. The indirectly related data cover possible analysis patterns, such as: (I) reproducibility in relation to author affiliation, (II) reproducibility related to conference and instalment year, and (III) reproducibility related to novelty of research.

\begin{description}
\item[Identifying information]: Includes recording of authors, the title and on-line link to paper.
\item[Miscellaneous data]: Data recorded to support analysis patterns and research characteristics.
    \begin{description}
    \item[Affiliation] Are the authors affiliated with an academic institution, or industry?
    \item[Conference] Notes the conference instalment the paper was published at.
    \item[Research type] Separates theoretical and experimental papers.
    \item[Result outcome] Does the paper present novel research?
    \item[Third-party citations] Are third-party software and data cited correctly?
    \end{description}
\item[Research transparency]: Explicit documentation of the research method in line with the scientific method.
    \begin{description}
    \item[Contribution] Clear description of what the research contributes.
    \item[Research goal or objective] Stated goals or objectives for the research.
    \item[Hypothesis] Stated hypothesis to investigate.
    \item[Prediction] Explicit mention of what the researchers predicted to see.
    \item[Problem description] An explicit mention of what the investigated problem is.
    \item[Research method] Description of the research method chosen.
    \item[Research question] Explicit listing of the research question(s) of interest.
    \end{description}
\end{description}

\subsection{Data generation method}
\label{sec:data-generation}
Data was generated by evaluating conference papers openly published in proceedings from two instalments of two different conference series. Physical copies can be ordered from the conferences, but all accepted papers are published on-line. This makes them easily available, and unobtrusive to obtain, allowing other researchers to scrutinize the research based on the original material.

The conference series investigated were the International Joint Conference on Artificial Intelligence (IJCAI) and the AAAI Conference on Artificial Intelligence (AAAI), specifically IJCAI-2013 and -2016, and AAAI-2014 and -2016. From these four instalments there are a combined population of 1910 accepted papers. IJCAI ran biennially until the first annual instalment in 2016, while AAAI was annual prior to 2014. Thus both conference series have had one instalment between the investigated years. Probabilistic random sampling was done for each conference, as documented in Appendix A\footnote{The sampling procedure is also available in a Jupyter notebook here: \url{https://github.com/sidgek/msoppgave}}. A sample size of 100 from each conference was selected, restricting the necessary time to conduct the survey. The sample generation creates a list of 100 papers, with any sub-slice being a valid random sample.

For IJCAI-2013 the 58 papers from \cite{Gundersen2015} were revisited, so only 42 of the 100 randomly sampled papers were included. This diminishes some of the representativeness of the IJCAI-2013 sample population. Due to the adapted survey, the papers were re-evaluated with the same procedure as the other papers.

The four conferences cover several disciplines within AI, and there may be differences within the disciplines. Between the conferences, however, it is assumed that the populations are not significantly different. This is based on the conferences covering the same disciplines at large, and employing blind peer review for acceptance with similar requirements in calls for papers. None of the conference series are vocal about open source or reproducible research. The AAAI conferences allow non-reviewed supplemental material, provided the documentation relevant for any claims is present in the paper itself, and the supplemental material adhere to the anonymity of the blind peer review. The IJCAI conferences do not allow supplemental material and discourage anonymized linking of supplemental material\footnote{See FAQ for ICJAI-16, the same language was used for IJCAI-13: \url{https://docs.google.com/document/d/1hOOvHxLyxSen5mnjVZOfta2Yb8XK3Fr-5rsF9Rfc9Ag/}}. The confidence interval for each conference is reported in table~\ref{tab:survey-confidence}, assuming that the populations are similar, a combined confidence interval is reduced to 4.36\%.

\begin{table}[!htb]
\begin{center}
    \begin{tabular}{  l | r  r  r }
    \textbf{Conference} & \textbf{Population Size} & \textbf{Sample Size} & \textbf{Confidence Interval} \\ \hline
    AAAI 2014 & 398 & 100 & 8.49 \\
    AAAI 2016 & 548 & 100 & 8.87 \\
    IJCAI 2013 & 413 & 100 & 8.54 \\
    IJCAI 2016 & 551 & 100 & 8.87 \\ \hline
    Combined & 1910 & 400 & 4.36 \\
    \end{tabular}
\end{center}
\caption[Confidence intervals of survey sample populations.]{Confidence intervals of survey sample populations given a 50/50 yes/no split with confidence level of 95\%. (\url{https://www.surveysystem.com/sscalc.htm})}
\label{tab:survey-confidence}
\end{table}

\section{Evaluation Procedure}
\label{sec:evaluation-procedure}
The following enumerated list, shows the procedure followed for each paper. Evaluating a single paper was estimated to take 10 to 12 minutes. Theoretical papers are considerably quicker, due to most of the variables not being of interest.

\begin{enumerate}
\item Note down the title, authors, link, and conference instalment for the paper.
\item Look at the institutions the authors are affiliated with. If unsure, look the institutions up on-line.
    \begin{enumerate}
    \item If the institutions are research institutions or academic institutions, record affiliation as 0.
    \item If industry institutions, record affiliation as 2.
    \item If there are institutions from both industry and academia, record affiliation as 1.
    \end{enumerate}
\item Skim the abstract.
    \begin{enumerate}
    \item Is the presented research novel? (Record Result outcome as 1 for yes, 0 for no)
    \item Does the abstract indicate an experiment? If not, scroll through the paper to look for an experiment. If no experiment is found record Research type as T for theoretical, if an experiment is found set E for experimental.
    \end{enumerate}
\item Search the paper file for explicit mentions of the following words: contribution, goal, objective, hypothesis, prediction, problem, research method, research question.
    \begin{enumerate}
    \item For each occurrence of a word, check the context it is mentioned in. If the context relates to the variables under \emph{Research transparency}, record that variable as 1. Otherwise, record them as 0.
    \end{enumerate}
\item If the \emph{Research type} was identified as theoretical, its evaluation is done and the remaining steps can be skipped.
\item Search through the paper.
    \begin{enumerate}
    \item If any pseudo-code is present, record pseudo-code as 1. Otherwise, 0.
    \item If any references to supplementary materials is made, look it up and see if the method is included.
    \item If any citations to datasets or source code are present, note third-party citations as 1. 0 otherwise.
    \end{enumerate}
\item The remaining variables are evaluated by skimming any sections identified as experimental or related to the experiment.
\item For mentions of datasets
    \begin{enumerate}
    \item If they are publicly available datasets or published by the authors, determine if any of the data is designated to training or validation. If neither set Open test data to 1, and the others to 0. If it is, set Open training data to 1.
    \item If Open training data was set to 1, look for specification of validation and test split of the data in the paper and at the referenced location of the data (some published datasets come with designated splits). Set Open validation and Open test data to 1 respectively if found. Otherwise set to 0.
    \item If any supplementary materials are referenced in the paper, look it up and see if the results data is available. Set to 1 if it is, 0 otherwise.
    \end{enumerate}
\item Look for mentions of CPU, RAM, GPU, AMD, Intel, GB.
    \begin{enumerate}
    \item Record Hardware specification as 1 if hardware is specified by model, 0 otherwise.
    Example of too little information: The experiment was run on a 4-core CPU. Accepted: The experiment was run on an Intel i5-4690K CPU at 3.5GHz.
    \end{enumerate}
\item Are any criteria to evaluate methods mentioned or discussed? Record 1 if yes, 0 if not.
\item Look for mentions of software dependencies.
    \begin{enumerate}
    \item If code is released, check for a requirements file or readme with requirements. If not see if there's any mention of software and its version in the paper. Set 1 if it is available, 0 if not. Note that including the version number is necessary.
    \end{enumerate}
\item Find any description of the experiment procedures themselves.
    \begin{enumerate}
    \item Are there procedures for running the experiment mentioned? Are hyper-parameters used for methods given or discussed? This may be documented in the source code. If found set experiment set-up to 1, otherwise 0.
    \end{enumerate}
\item If any supplementary materials are referenced, see if they include code to run the experiment. Set open experiment code to 1 if they are, 0 otherwise.
\end{enumerate}

Refer to Appendix B\footnote{The full dataset is also available at \url{https://github.com/sidgek/msoppgave}} for an abbreviated sample of how the survey data is structured. The data was recorded in a Google spreadsheet, and exported to a csv file.

\section{Limitations of the Survey}
Hardware specifications and software dependencies are difficult to evaluate. For software dependencies, it is difficult to specify software versions in a paper without sacrificing valuable paper space. If the code is not released, the value of it is low. For released code it is common best-practices to have a requirements file along with the code, it is highly recommended to do so. Hardware specification is difficult to set baselines for, as some experiments may be run through cloud services, or on multiple devices it can be difficult to be precise. Additionally, depending on the experiment and implementation, the hardware might not be particularly relevant. It does however give an indication of the resources necessary to perform the experiment. If speed and performance is an important factor in a paper, ranking of different methods is more valuable than exact speed, but it might still be valuable to discuss how the resources available to the methods impact the ranking.

Similarly, for experiment set-up, how detailed the description needs to be varies. A baseline for how detailed it is necessary to be for reproduction depends heavily on the reader. Ideally code to run the experiment is available which allows other researchers to examine the set-up. The paper should still include enough information on the steps done during the experiment for someone familiar with the field to recognize. During evaluations for the survey, experiment set-up became a check for discussion of hyper-parameters rather than experiment procedures. This shows an example of how evaluation bias can impact the survey, where a variable was modified to the investigators meaning rather than the intended.

The survey does not take licensing into account when evaluating the availability of data and code. Evaluations noted to make data and code available, may restrict the use without any indication in the evaluation data.

While it is intended, it is also important to repeat that the survey does not show successful attempts at reproduction. It merely investigates the availability of materials deemed necessary to attempt a reproduction. While such attempts would be valuable and interesting, the cost is substantial compared to the cost of the survey. Such attempts would likely require more manpower or a reduced sample size, but would also give a more precise indication of whether enough information is provided in a paper.
\cleardoublepage
