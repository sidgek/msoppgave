%===================================== CHAP 3 =================================

\chapter{Research Method}

The survey designed to investigate the state of reproducibility at AI conferences is based on manually analysing research papers and marking
Investigating the reproducibility of research papers has previously been done by attempting to run experiments with
Analysing the reproducibility of experiments based on a paper have previously been done

\section{Literature Survey Design}

Advantages of doing a survey
- Can be replicated on similar documents or on original documents provided the method is shared and documents are accessible
- Can produce a lot of data at a low cost, in a relatively short time compared to attempting full replications of experiments
- Allows a larger sample population due to the shorter time necessary to evaluate a paper

Disadvantages
- The depth is restricted, does not provide detail on the research topic
- Focuses on what can be counted and measured, other aspects may be overlooked

\subsection{Data requirements}
\label{subsec:data-req}

The following variables were recorded for each paper. They are here divided into two sections: directly, and indirectly topic related. The directly topic related variables are categorised into: Experiment documentation, documentation and availability of the experiment; Method documentation, documentation and availability of the method presented; and Open Data, documentation and availability of data used. The indirectly topic related variables are categorised into: Miscellaneous, meta-data describing the research; and Research Transparency, documentation and openness of the research method.

\subsubsection{Directly topic related}
\begin{description}
\item[Experiment documentation]: How well documented is the experiment and the environment it was performed in.
    \begin{description}
    \item[Open experiment code] Is the code to run the experiment made available?
    \item[Hardware specification] Is the hardware used during the experiment specified?
    \item[Software dependencies] Are software dependencies listed?
    \item[Experiment set-up] Is the set-up for the experiment described? Are hyper-parameters used during the experiment specified?
    \item[Evaluation criteria] Are the criteria used to evaluate the method described?
    \end{description}
\end{description}
/* Explain variables and relate to best practices!!! */
\begin{description}
\item[Directly topic related]:
        Is source code or data open for the experiment and method?
        Is the method documented?
        Is the experiment documented?
        etc.
\item[Indirectly topic related]
        Research transparency (hypothesis, predictions...)
        Author affiliation (uni/industry/both)
        Novel research?
        Conference view on supplementary material
        Theoretical / Experimental research

\end{description}

Possible analysis patterns
\begin{enumerate}
    \item reproducibility related to author affiliation
    \item reproducibility related to conference view on supplementary material?
    \item reproducibility related to publishing year (improvement over time?)

\end{enumerate}

\subsection{Data generation method}
Data will be generated by evaluating conference papers openly published in proceedings from two instalments of two different conferences. Physical copies can be ordered from the conferences, but all accepted papers are freely available on-line. This makes them easily available, and unobtrusive to obtain. Additionally, it allows other researchers to scrutinize the research based on original material.

The conferences investigated were the International Joint Conference on Artificial Intelligence (IJCAI) and the AAAI Conference on Artificial Intelligence (AAAI), specifically IJCAI-2013 and -2016, and AAAI-2014 and -2016. From these four instalments there are a combined population of 1910 accepted papers. A sample size of 100 from each conference was selected, restricting the necessary time to conduct the survey. Confidence intervals are reported in table~\ref{tab:survey-confidence}. Probabilistic random sampling of each conference separately was done, as documented in Appendix A\footnote{The sampling procedure is also available in a Jupyter notebook here: \url{https://github.com/sidgek/msoppgave}}.

Sampling frame: accepted papers at IJCAI-13, -16 and AAAI-14 and -16 (can be seen in repo files for sample generation)
Sampling technique: probabilistic random sampling of each conference separately.
        "Probability sampling, as its name suggests, means that the sample has been chosen because the researcher believes that there is a high probability that the sample of respondents (or events) chosen are representative of the overall population being studied. That is, they form a representative cross-section of the overall population." Oates p.96

Sample size: 100 for each conference, restricts the necessary time to conduct the survey while still providing informative accuracy ranges when considering previous research (cite?)

\begin{table}[!h]
\begin{center}
    \begin{tabular}{  l | r  r  r }
    \textbf{Conference} & \textbf{Population Size} & \textbf{Sample Size} & \textbf{Confidence Interval} \\ \hline
    AAAI 2014 & 398 & 100 & 8.49 \\
    AAAI 2016 & 548 & 100 & 8.87 \\
    IJCAI 2013 & 413 & 100 & 8.54 \\
    IJCAI 2016 & 551 & 100 & 8.87 \\ \hline
    Combined & 1910 & 400 & 4.36 \\
    \end{tabular}
\end{center}
\caption{Confidence intervals of survey sample populations given a 50/50  yes/no split with confidence level of 95\%. (\url{https://www.surveysystem.com/sscalc.htm})}
\label{tab:survey-confidence}
\end{table}

The four conferences cover several disciplines within AI, and there may be differences within the disciplines. Between the conferences, however, it is assumed that the populations are not significantly different. This is based on the conferences covering the same disciplines at large, and employing blind peer review for acceptance. None of the conferences are vocal about open source or reproducible research, though the AAAI conferences allow non-reviewed supplemental material provided the documentation relevant for any claims is present in the paper itself. The confidence interval for each conference is reported in table~\ref{tab:survey-confidence}, assuming that the populations are similar, a combined confidence interval is reduced to 4.36\%.

\section{Evaluation Procedure}
\label{sec:evaluation-procedure}
- Step by step 'instructions'
- Sampling documentation
- Evaluation documentation
- Example evaluations (variable X: "Exhibit A" covers, "Exhibit B" is not enough)

\section{Limitations of the Survey}
- Evaluation bias (modification of variables)
- Sample inconsistency for IJCAI-13 (~50 papers)
- Not an actual attempt at reproducing experiments, researcher's view that discussion of a variable is missing?

\cleardoublepage
