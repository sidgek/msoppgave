\pagenumbering{roman} 				
\setcounter{page}{1}

\pagestyle{fancy}
\fancyhf{}
\renewcommand{\chaptermark}[1]{\markboth{\chaptername\ \thechapter.\ #1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\renewcommand{\headrulewidth}{0.1ex}
\renewcommand{\footrulewidth}{0.1ex}
\fancyfoot[LE,RO]{\thepage}
\fancypagestyle{plain}{\fancyhf{}\fancyfoot[LE,RO]{\thepage}\renewcommand{\headrulewidth}{0ex}}

\section*{\Huge Abstract}
\addcontentsline{toc}{chapter}{Abstract}	
$\\[0.5cm]$

\noindent Reproducibility of published computational research has seen increased interest the last twenty years. Regardless of academic field and the impact-factor of journals, studies of reproducibility of computational research have found low rates of reproducibility. Common issues relate to the availability of source code and data, even when original authors attempt to reproduce their own published research.

In this thesis, we investigate the state of reproducibility in artificial intelligence research. The objective is not to reproduce experiments, but to investigate and quantify the state of reproducibility in artificial intelligence research. Two hypotheses were investigated: 1) Documentation of AI research is not good enough to reproduce results, and 2) Documentation practices have improved in recent years. 400 research papers from two instalments of two top AI conference series, IJCAI and AAAI, have been surveyed to investigate the hypotheses. The results of our survey support the first hypothesis, but not the second. While common usage of public datasets is widespread, sharing of code is lagging behind. Facilitating sharing of source code, and data without disrupting the peer review process are necessary to improve the situation.

The contribution efforts of the research in this thesis are: (i) a survey design for evaluating documentation of published papers, (ii) an evaluation of two leading AI conference series, and (iii) suggested incentives to facilitate the reproducibility of AI research.


\cleardoublepage

\section*{\Huge Sammendrag}
\addcontentsline{toc}{chapter}{Sammendrag}
$\\[0.5cm]$

\noindent Reproduserbarhet av publisert forskning har sett økende interesse og diskusjon de siste årene. Studier som undersøker reproduserbarhet har gjentatte ganger vist lav grad av reproduserbarhet innen flere akademiske felt, uavhengig av innflytelsesfaktoren til journalen. Ofte diskuterte problemer relaterer til hvor tilgjengelig kildekode og data benyttet er, selv for de opprinnelige forskerne.

I denne oppgaven ser vi på status for reproduserbarhet for publisert forskning innen kunstig intelligens. Målet er å undersøke og kvantifisere reproduserbarheten innen kunstig intelligens. To hypoteser ble gransket: 1) Publiseringer innen kunstig intelligens dokumenterer ikke nok til å tillate reprodusering, og 2) Dokumentasjonspraksis har bedret seg de siste årene. For å forske på hypotesene har en undersøkelse av 400 publiserte artikler fra to ledende konferanseserier innen kunstig intelligens, IJCAI og AAAI, blitt utført. To utgaver av hver konferanseserie har blitt undersøkt. Resultatene støtter kun den første hypotesen. Bruk av åpne datasett er utbredt, men deling av kildekode ser ut til å henge etter. For å forbedre reproduserbarheten til konferansene, vil intensiver til å dele kildekode, programmer, og data uten å påvirke peer-review prosessen være nødvendig.

Forskningen presentert i denne oppgaven har resultert i følgende bidrag: (i) design av reproduserbarhetsundersøkelse for publiserte artikler, (ii) en evaluering av to ledende konferanseserier, og (iii) forslag til intensiver som kan øke reproduserbarheten innen kunstig intelligens.

\cleardoublepage